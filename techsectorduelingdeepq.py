# -*- coding: utf-8 -*-
"""TechSectorDuelingDeepQ.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1wnPU5SenKP6GcS2Od29QRCgIBqwCpoSz
"""

!nvidia-smi

"""## Creating model, agent and memory management classes"""

# dueling_dqn_lstm.py
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import load_model
import numpy as np

class DuelingDeepQNetwork(keras.Model):
    # fc = fully connected, fc_dims: number of units, neurons
    def __init__(self, n_actions, fc1_dims, fc2_dims):
        super(DuelingDeepQNetwork, self).__init__()

        keras.backend.set_floatx('float64')
        self.leaky_relu1 = keras.layers.LeakyReLU()
        self.lstm1 = keras.layers.LSTM(units=64, input_shape=(64,2), return_sequences=True, dtype='float64', activation=None)
        self.leaky_relu2 = keras.layers.LeakyReLU()
        self.lstm2 = keras.layers.LSTM(units=64, input_shape=(64,2), return_sequences=True, dtype='float64', activation=None)
        self.V = keras.layers.Dense(1, activation=None)
        self.A = keras.layers.Dense(n_actions, activation=None)

        print('Network created')

    def call(self, state, training=True):
        x = self.leaky_relu1(state)
        x = self.lstm1(x)
        x = self.leaky_relu2(x)
        x = self.lstm2(x)

        # define behaviour during training
        if training == True:
            V = self.V(x)
            A = self.A(x)

            Q = (V + (A - tf.math.reduce_mean(A, axis=1, keepdims=True)))

            return Q

        # during testing i only need the advantage model
        else:
            A = self.A(x)

            return A

    @tf.function
    def advantage(self, state):
        x = self.leaky_relu1(state)
        x = self.lstm1(x)
        x = self.leaky_relu2(x)
        x = self.lstm2(x)
        A = self.A(x)

        return A

class ReplayBuffer():
    def __init__(self, max_size, input_shape):
        self.mem_size = max_size
        self.mem_cntr = 0

        self.state_memory = np.zeros((self.mem_size, *input_shape),
                                        dtype=np.float64)
        self.new_state_memory = np.zeros((self.mem_size, *input_shape),
                                        dtype=np.float64)
        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)
        self.reward_memory = np.zeros(self.mem_size, dtype=np.float64)
        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)

    def store_transition(self, state, action, reward, state_, done):
        index = self.mem_cntr % self.mem_size
        self.state_memory[index] = state
        self.new_state_memory[index] = state_
        self.action_memory[index] = action
        self.reward_memory[index] = reward
        self.terminal_memory[index] = done

        self.mem_cntr += 1
        

    def sample_buffer(self, batch_size):
        max_mem = min(self.mem_cntr, self.mem_size)
        batch = np.random.choice(max_mem, batch_size, replace=False)

        states = self.state_memory[batch]
        new_states = self.new_state_memory[batch]
        actions = self.action_memory[batch]
        rewards = self.reward_memory[batch]
        dones = self.terminal_memory[batch]

        return states, actions, rewards, new_states, dones

class Agent():
    def __init__(self, lr, gamma, n_actions, epsilon, batch_size, input_dims, epsilon_dec=1e-3, eps_end=0.01, 
                 mem_size=10000, fname='model', fc1_dims=128, fc2_dims=128, replace=100, testing=False, model=None):

        if testing == True:
            self.action_space = np.array([-1,0,1])
            self.gamma = gamma
            self.epsilon = epsilon
            self.eps_dec = epsilon_dec
            self.eps_min = eps_end
            self.fname = fname
            self.replace = replace
            self.batch_size = batch_size

            self.learn_step_counter = 0
            self.memory = ReplayBuffer(mem_size, input_dims)
            self.q_eval = model
            self.q_next = model

            # set learning rate and optimizer
            self.q_eval.compile(optimizer=Adam(learning_rate=lr),
                                loss='mean_squared_error')
            # just a formality, won't optimize network
            self.q_next.compile(optimizer=Adam(learning_rate=lr),
                                loss='mean_squared_error')
            
            # keeping track of chosen actions
            self.chosen_actions = []

            print('Testing agent created')

        else:

            # action space definition [-1,0,1], ezt az [i for i in range(n_actions)]-t majd ki kell venni, n_actions = 3
            self.action_space = np.array([-1,0,1])
            self.gamma = gamma
            self.epsilon = epsilon
            self.eps_dec = epsilon_dec
            self.eps_min = eps_end
            self.fname = fname
            self.replace = replace
            self.batch_size = batch_size

            self.learn_step_counter = 0
            self.memory = ReplayBuffer(mem_size, input_dims)
            self.q_eval = DuelingDeepQNetwork(n_actions, fc1_dims, fc2_dims)
            self.q_next = DuelingDeepQNetwork(n_actions, fc1_dims, fc2_dims)

            # set learning rate and optimizer
            self.q_eval.compile(optimizer=Adam(learning_rate=lr),
                                loss='mean_squared_error')
            # just a formality, won't optimize network
            self.q_next.compile(optimizer=Adam(learning_rate=lr),
                                loss='mean_squared_error')
            
            # keeping track of chosen actions
            self.chosen_actions = []

            print('Training agent created')
        
        
    def store_transition(self, state, action, reward, new_state, done):
        self.memory.store_transition(state, action, reward, new_state, done)

    def choose_action(self, observation, testing=False):
        if testing == False:
            if np.random.random() < self.epsilon:
                action = np.random.choice(self.action_space)
                # converting to native Python type
                action = action.item()
                print('actionE:', action)

                action_dicti = {'Epsilon': action}
                self.chosen_actions.append(action_dicti)
            else:
                # state = np.array([observation])
                state = observation
                actions = self.q_eval.advantage(state)
                action = tf.math.argmax(actions, axis=1).numpy()[0]

                # get index of max value inside ndarray
                max_idx = np.where(action == np.amax(action))

                action_idx = 0

                # max_idx[0] is a 'list' (1D ndarray)

                if len(max_idx[0] > 1):
                    action_idx = int(np.random.choice(max_idx[0], 1))
                else:
                    action_idx = int(max_idx[0])
                    
                if action_idx == 0:
                    action = -1
                elif action_idx == 1:
                    action = 0
                elif action_idx == 2:
                    action = 1
                print('actionM:', action)

                action_dicti = {'Model': action}
                self.chosen_actions.append(action_dicti)
        
        # ha tesztelés van, akkor rögtön amodellbe küldje a state-t
        elif testing == True:
            # state = np.array([observation])
            state = observation
            actions = self.q_eval.advantage(state) # advantage volt itt eredetileg
            action = tf.math.argmax(actions, axis=1).numpy()[0]

            # get index of max value inside ndarray
            max_idx = np.where(action == np.amax(action))

            # lets see if max_idx consist of multiple elements or just one
            # if it has multiple element than i choose one randomly
            action_idx = 0

            # max_idx[0] is a 'list' (1D ndarray)

            if len(max_idx[0] > 1):
                action_idx = int(np.random.choice(max_idx[0], 1))
            else:
                action_idx = int(max_idx[0])

            # na nézzük a pozíciókat, indexet (0,1,2)
            if action_idx == 0:
                action = -1
            elif action_idx == 1:
                action = 0
            elif action_idx == 2:
                action = 1
            print('actionM:', action)

            action_dicti = {'Model': action}
            self.chosen_actions.append(action_dicti)
            

        return action

    def learn(self):
        if self.memory.mem_cntr < self.batch_size:
            return

        if self.learn_step_counter % self.replace == 0:
            self.q_next.set_weights(self.q_eval.get_weights())

        states, actions, rewards, states_, dones = self.memory.sample_buffer(self.batch_size)

        q_pred = self.q_eval(states)
        q_next = tf.math.reduce_max(self.q_next(states_), axis=1, keepdims=True).numpy()
        q_target = np.copy(q_pred)

        for idx, terminal in enumerate(dones):
            if terminal:
                q_next[idx] = 0.0
            q_target[idx, actions[idx]] = rewards[idx] + self.gamma*q_next[idx]

        self.q_eval.train_on_batch(x=states, y=q_target) # states

        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min

        self.learn_step_counter += 1

    def reward_function(self, a_t1, sigma_tgt, sigma_t1, r_t, bp, p_t1, sigma_t2, a_t2):
        R_t = a_t1 * (sigma_tgt/sigma_t1) * r_t - bp * p_t1 * abs((sigma_tgt/sigma_t1) * a_t1 - (sigma_tgt/sigma_t2) * a_t2)

        return R_t

    def save_model(self):
        # define full path for model to save to
        self.q_eval.save(self.fname, save_format='tf', overwrite=True)

    def load_model(self):
        self.q_eval = load_model(self.fname)
        return self.q_eval

"""## Ticker and data index settings """

# whether i want to use combined dataset or not
combined = True

if combined:
    ticker = 'tech_combined'
    data_index = 115840

    # limit - minus_value: az adott ticker indexének dinamikus megtalálása
    idx = 0 # AAPL
    batch_size_basic = 64
    minus_value = batch_size_basic - idx
else:
    ticker = 'AMD'
    data_index = 3968
    minus_value = 1

print(ticker)
print(data_index)
print(minus_value)

"""## Getting data, preprocessing"""

# main_keras_dueling_dqn_lstm.py
import pandas as pd
import numpy as np
from sklearn import preprocessing
import math
from statistics import median, mean

# connect to Drive
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# AAPL
# data_link = 'https://drive.google.com/file/d/1YSTQDwmzZzLoUd8O9gAy6_2JOo7R2qRx/view?usp=sharing'
# id = '1YSTQDwmzZzLoUd8O9gAy6_2JOo7R2qRx'

# VOO
# data_link = 'https://drive.google.com/file/d/15xvN9J4iaFPmgzqAbxkrdCqja3zpJnsK/view?usp=sharing'
# id = '15xvN9J4iaFPmgzqAbxkrdCqja3zpJnsK'

# AMZN
# data_link = 'https://drive.google.com/file/d/1o54V2UYiZrKgBwxgQ1_5Y470mboSbhyC/view?usp=sharing'
# id = '1o54V2UYiZrKgBwxgQ1_5Y470mboSbhyC'

# AMD
# data_link = 'https://drive.google.com/file/d/14YifDjDJqF89U6jjVy9S_Sdu-yiy-gdj/view?usp=sharing'
# id = '14YifDjDJqF89U6jjVy9S_Sdu-yiy-gdj'
# print(id)

# combined
# data_link = 'https://drive.google.com/file/d/1BGLrDsIU2H45mX4dNnffUCte8vzUaLqg/view?usp=sharing'
# id = '1BGLrDsIU2H45mX4dNnffUCte8vzUaLqg'
# print(id)

# tech_combined
data_link = 'https://drive.google.com/file/d/1pTq20b93mGVfyi2fmHhUZj8j2-7TTBHu/view?usp=sharing'
id = '1pTq20b93mGVfyi2fmHhUZj8j2-7TTBHu'
print(id)

downloaded = drive.CreateFile({'id':id}) 

if combined:
    print('...combined data chosen...')
    data_file = 'tech_combined.csv'
    downloaded.GetContentFile(data_file)

    # iterate csv file and getting train batches
    data = pd.read_csv(data_file)

    data_train = pd.DataFrame(data[:data_index])
    data_train = data_train.sort_values(by=['date', 'ticker'], ascending=(True, True))
else:
    print('...conventional individual stock data chosen...')
    data_file = ticker + '_Yahoo.csv'
    downloaded.GetContentFile(data_file)

    # iterate csv file and getting train batches
    data = pd.read_csv(data_file)

    # data[:1792] for VOO
    data_train = pd.DataFrame(data[:data_index])
    data_train = data_train.sort_values(by=['id'], ascending=True)

# normalize train data by adding normalized columns
data_normalizer = preprocessing.MinMaxScaler()

close_array = np.array(data_train['close'])
close_reshaped = close_array.reshape(-1, 1)

data_train['close_normalized'] = data_normalizer.fit_transform(close_reshaped)

volume_array = np.array(data_train['volume'])
volume_reshaped = volume_array.reshape(-1, 1)

data_train['volume_normalized'] = data_normalizer.fit_transform(volume_reshaped)

# adding helper columns for usage in reward function - only using the normalized values
data_train['r_t'] = data_train['close_normalized'].diff().fillna(0)
data_train['sigma_t'] = data_train['r_t'].ewm(span=60).std().fillna(0)

print(data_train.head())
print(data_train.tail())

'''
# normalize train data by adding normalized columns
data_normalizer = preprocessing.MinMaxScaler()
close_array = np.array(data_train['close'])
close_reshaped = close_array.reshape(-1, 1)
data_train['close_normalized'] = data_normalizer.fit_transform(close_reshaped)
volume_array = np.array(data_train['volume'])
volume_reshaped = volume_array.reshape(-1, 1)
data_train['volume_normalized'] = data_normalizer.fit_transform(volume_reshaped)
# adding helper columns for usage in reward function - only using the normalized values
data_train['r_t'] = data_train['close_normalized'].diff().fillna(0)
data_train['sigma_t'] = data_train['r_t'].ewm(span=60).std().fillna(0)
print('head of data_train:')
print(data_train.head())
'''

"""## Instantiate training agent"""

# change model name if data changes
# model_name = '/content/gdrive/My Drive/Colab Notebooks/saved_files/model_' + ticker
model_name = '/content/gdrive/My Drive/Colab Notebooks/saved_files/model_tech_combined'
print(model_name)

agent = Agent(lr=0.0001, gamma=0.3, n_actions=3, epsilon=1, batch_size=64, input_dims=(64,2), 
              epsilon_dec=1e-3, eps_end=0.01, mem_size=5000, fname=model_name,
              fc1_dims=64, fc2_dims=64, replace=1000)

"""## Helper variables and constants"""

eps_history = []

# constants
sigma_tgt = 0.03
reward = 0
L = len(data_train)

print('L:', L)

done = False
round = 0

# actual price: close * (1-bp)
bp = 0.001

transactions = pd.DataFrame(columns=['action', 'close', 'value', 'cost', 'qty', 'crt_blc', 'round'])

# budget 
current_balance = 100000

"""testing"""

'''
batch_start = 500
batch_end = 564
batch_raw = data_train[batch_start:batch_end]
# filter
batch = batch_raw[['close_normalized', 'volume_normalized']]
# okkay, i got the data - the state
# convert it to numpy array, then it becomes the observation
observation = np.array(batch)
observation_3d = observation.reshape(1, 64, 2)
# berakni a modellbe, várni az outputot
# action = agent.choose_action(observation_3d)
actions = agent.q_eval.advantage(observation_3d)
action = tf.math.argmax(actions, axis=1).numpy()[0]
print('ndarray action:', action)
# get index of max value inside ndarray
max_idx = np.where(action == np.amax(action))
# lets see if max_idx consist of multiple elements or just one
# if it has multiple element than i choose one randomly
action_idx = 0
# max_idx[0] is a 'list' (1D ndarray)
if len(max_idx[0] > 1):
    action_idx = int(np.random.choice(max_idx[0], 1))
else:
    action_idx = int(max_idx[0])
# na nézzük a pozíciókat, indexet (0,1,2)
if action_idx == 0:
    action = -1
elif action_idx == 1:
    action = 0
elif action_idx == 2:
    action = 1
print('Muthafuckin action is:', action)
'''

"""## Training model"""

batch_start = 0
batch_end = agent.batch_size

while batch_end <= L:
    limit = min(batch_end, L) 

    print('batch_end:', batch_end)
    print('round:', round)
    
    # ha a limit az L-t választja, akkor azt jelenti, hogy végig értünk az adathalmazon, done = True
    # we don't learn here
    if limit == L:
        print('limit:', limit, 'L:', L)
        batch_raw = data_train[batch_start:limit]

        # filter
        batch = batch_raw[['close_normalized', 'volume_normalized']]

        # convert it to numpy array, then it becomes the observation
        observation = np.array(batch)
        observation_3d = observation.reshape(1, 64, 2)

        action = agent.choose_action(observation_3d)
        print('action in non-learning:', action)

        close_median = batch_raw['close'].median()

        # megvan az action, kezeljük le ([-1,0,1])

        # sell
        if action == -1:
            # done = True
            transactions = transactions.sort_values(by=['round'], ascending=True)
            last_sell_index = transactions.loc[transactions['action'] == -1].last_valid_index()

            close = 0
            value = 0
            cost = 0
            qty = 0

            if last_sell_index == None:
                valid_records = transactions.loc[transactions['action'] == 1]

                if valid_records.empty:
                    #close = data_train['close'].iloc[limit-minus_value]
                    #print(data_train['ticker'].iloc[limit-minus_value])
                    close = close_median
                    value = 0
                    cost = 0
                    qty = 0
                else:
                    # close = (valid_records['close'] * valid_records['qty']).sum()
                    # qty = valid_records['qty'].sum()
                    #close = data_train['close'].iloc[limit-minus_value]
                    #print(data_train['ticker'].iloc[limit-minus_value])
                    close = close_median
                    qty = valid_records['qty'].sum()
                    value = qty * close
                    cost = value * bp
                    
                    current_balance = current_balance + (value - cost)

                    # profit += close


            else:
                valid_interval_data = transactions.iloc[last_sell_index:len(transactions)]

                # filter valid_interval_data where action == 1
                valid_records = valid_interval_data.loc[valid_interval_data['action'] == 1]

                if valid_records.empty:
                    close = 0
                    value = 0
                    cost = 0
                    qty = 0
                else:
                    # close = (valid_records['close'] * valid_records['qty']).sum()
                    # qty = valid_records['qty'].sum()
                    #close = data_train['close'].iloc[limit-minus_value]
                    #print(data_train['ticker'].iloc[limit-minus_value])
                    close = close_median
                    qty = valid_records['qty'].sum()
                    value = qty * close
                    cost = value * bp
                    
                    current_balance = current_balance + (value - cost)

                    # profit += close

            # okkay do dict
            dicti = {'action': action, 'close': close, 'value': value, 'cost': cost, 'qty': qty, 'crt_blc': current_balance, 'round': round}

            # convert dict to pandas Series
            s = pd.Series(dicti)

            # add Series to transactions df - df = df.append
            transactions = transactions.append(s, ignore_index=True)
            
            #print('transactions')
            #print(transactions)

            # # calculate reward - this is far from finished i think
            # reward = agent.reward_function(a_t1=transactions.iloc[len(transactions)-2] if len(transactions) >= 2 else 0, sigma_tgt=sigma_tgt, sigma_t1=data_train['sigma_t'].iloc[limit-2], r_t=data_train['r_t'].iloc[limit-1], bp=0.0020, p_t1=data_train['close'].iloc[limit-2], sigma_t2=data_train['sigma_t'].iloc[limit-3], a_t2=transactions.iloc[len(transactions)-3] if len(transactions) >= 3 else 0)

            # done = True

            # # run state through model
            # agent.learn()

        # hold
        elif action == 0:
              # get current number of stocks (qty i hold = hold_qty)
            # get index of last sell
            transactions = transactions.sort_values(by=['round'], ascending=True)
            last_sell_index = transactions.loc[transactions['action'] == -1].last_valid_index()
            hold_qty = 0

            # close = value of held stocks
            close = 0
            value = 0
            cost = 0

            # last_sell_index == None: nem volt még eladás, akkor mi legyen
            if last_sell_index == None:
                # ha nincsen sell, akkor minden okés, az egész transactionst filterezzük
                valid_records = transactions.loc[transactions['action'] == 1]
                
                # ha nincsen buy
                if valid_records.empty:
                    hold_qty = 0
                    #close = data_train['close'].iloc[limit-minus_value]
                    # print(data_train['ticker'].iloc[limit-minus_value])
                    close = close_median
                    value = 0
                    cost = 0
                else:
                    # hold_qty = valid_records['qty'].sum()
                    # close = (valid_records['close'] * valid_records['qty']).sum()
                    hold_qty = valid_records['qty'].sum()
                    #close = data_train['close'].iloc[limit-minus_value]
                    #print(data_train['ticker'].iloc[limit-minus_value])
                    close = close_median
                    value = valid_records['value'].sum()
                    cost = 0
            else:
                # else: ha volt eladás, akkor mi legyen
                
                # last index in df might be: df.last_valid_index()
                # more logic needed
                # disregard hold actions, only count buys
                # hold_qty = transactions['qty'].iloc[last_sell_index:len(transactions)-1].sum(axis=1)
                valid_interval_data = transactions.iloc[last_sell_index:len(transactions)]

                # filter valid_interval_data where action == 1
                valid_records = valid_interval_data.loc[valid_interval_data['action'] == 1]

                # ha nincsen buy 
                if valid_records.empty:
                    hold_qty = 0
                    #close = data_train['close'].iloc[limit-minus_value]
                    #print(data_train['ticker'].iloc[limit-minus_value])
                    close = close_median
                    value = 0
                    cost = 0
                else:
                    # get sum of these records' qty
                    # hold_qty = valid_records['qty'].sum()
                    # close = (valid_records['close'] * valid_records['qty']).sum()
                    hold_qty = valid_records['qty'].sum()
                    #close = data_train['close'].iloc[limit-minus_value]
                    #print(data_train['ticker'].iloc[limit-minus_value])
                    close = close_median
                    value = hold_qty * close
                    cost = 0

            # dict to record action parameters
            dicti = {'action': action, 'close': close, 'value': value, 'cost': cost, 'qty': hold_qty, 'crt_blc': current_balance, 'round': round}

            # convert dict to pandas Series
            s = pd.Series(dicti)

            # add Series to transactions df - df = df.append
            transactions = transactions.append(s, ignore_index=True)

        # buy
        elif action == 1:
            #close = data_train['close'].iloc[limit-minus_value]
            #print(data_train['ticker'].iloc[limit-minus_value])
            close = close_median

            buy_qty = math.floor((current_balance / 5) / close)
            value = buy_qty * close
            cost = value * bp

            current_balance = current_balance - (value + cost)

            # dict to record action parameters
            dicti = {'action': action, 'close': close, 'value': value, 'cost': cost, 'qty': buy_qty, 'crt_blc': current_balance, 'round': round}

            # convert dict to pandas Series
            s = pd.Series(dicti)

            # add Series to transactions df - df = df.append
            transactions = transactions.append(s, ignore_index=True)

        break
    
    # a limit belefér az adathalmazban itt folytathatjuk a batch-et, done = False
    else:
        batch_raw = data_train[batch_start:limit]

        # filter
        batch = batch_raw[['close_normalized', 'volume_normalized']]

        # okkay, i got the data - the state
        # convert it to numpy array, then it becomes the observation
        observation = np.array(batch)
 
        observation_3d = observation.reshape(1, 64, 2)

        # berakni a modellbe, várni az outputot
        action = agent.choose_action(observation_3d)
        print('action in learning:', action)

        # talán itt is megtudjuk oldani a closet
        close_median = batch_raw['close'].median()

        # sell
        if action == -1:
            transactions = transactions.sort_values(by=['round'], ascending=True)

            last_sell_index = transactions.loc[transactions['action'] == -1].last_valid_index()
            #print('last_sell_index:', last_sell_index)

            close = 0
            value = 0
            cost = 0
            qty = 0

            # még nem volt eladás
            if last_sell_index == None:
                valid_records = transactions.loc[transactions['action'] == 1]
                
                # nincsen vétel sem
                if valid_records.empty:
                    #close = data_train['close'].iloc[limit-minus_value]
                    #print(data_train['ticker'].iloc[limit-minus_value])
                    close = close_median
                    value = 0
                    cost = 0
                    qty = 0
                else:
                    # close = (valid_records['close'] * valid_records['qty']).sum()
                    #close = data_train['close'].iloc[limit-minus_value]
                    #print(data_train['ticker'].iloc[limit-minus_value])
                    close = close_median
                    qty = valid_records['qty'].sum()
                    value = qty * close
                    cost = value * bp
                    
                    current_balance = current_balance + (value - cost)

                    # profit += close


            else:
                valid_interval_data = transactions.iloc[last_sell_index:len(transactions)] #-1

                # filter valid_interval_data where action == 1
                valid_records = valid_interval_data.loc[valid_interval_data['action'] == 1]

                if valid_records.empty:
                    close = 0
                    value = 0
                    cost = 0
                    qty = 0
                else:
                    # close = (valid_records['close'] * valid_records['qty']).sum()
                    #close = data_train['close'].iloc[limit-minus_value]
                    #print(data_train['ticker'].iloc[limit-minus_value])
                    close = close_median
                    qty = valid_records['qty'].sum()
                    value = qty * close
                    cost = value * bp
                    
                    current_balance = current_balance + (value - cost)

                    # profit += close

            # okkay do dict
            dicti = {'action': action, 'close': close, 'value': value, 'cost': cost, 'qty': qty, 'crt_blc': current_balance, 'round': round}

              # convert dict to pandas Series
            s = pd.Series(dicti)

            # add Series to transactions df - df = df.append
            transactions = transactions.append(s, ignore_index=True)

            # calculate reward - this is far from finished i think
            reward = agent.reward_function(a_t1=transactions['action'].iloc[len(transactions)-2] if len(transactions) >= 2 else 0, sigma_tgt=sigma_tgt, sigma_t1=data_train['sigma_t'].iloc[limit-2], r_t=data_train['r_t'].iloc[limit-1], bp=0.0020, p_t1=data_train['close'].iloc[limit-2], sigma_t2=data_train['sigma_t'].iloc[limit-3], a_t2=transactions['action'].iloc[len(transactions)-3] if len(transactions) >= 3 else 0)
            print('reward 0')
            print(reward)
            # reward = float(reward)

            done = False

            # false new observation_ for memory management
            false_batch_start = batch_start + 64
            false_batch_end = batch_end + 64

            false_limit = min(false_batch_end, L)

            false_batch_raw = data_train[false_batch_start:false_limit]

            false_batch = false_batch_raw[['close_normalized', 'volume_normalized']]
            
            observation_ = np.array(false_batch)
            observation_3d_ = observation_.reshape(1, 64, 2)

            # store transition
            agent.store_transition(observation_3d, action, reward, observation_3d_, done)

            # run state through model
            agent.learn()

            # reset false_batch_raw
            false_batch_raw = false_batch_raw.drop(false_batch_raw.index, inplace=True)
            false_batch_start = 0
            false_batch_end = 0

        # hold
        elif action == 0:
            transactions = transactions.sort_values(by=['round'], ascending=True)
            last_sell_index = transactions.loc[transactions['action'] == -1].last_valid_index()
            hold_qty = 0

            # close = value of held stocks
            close = 0
            value = 0
            cost = 0

            # last_sell_index == None: nem volt még eladás, akkor mi legyen
            if last_sell_index == None:
                # ha nincsen sell, akkor minden okés, az egész transactionst filterezzük
                valid_records = transactions.loc[transactions['action'] == 1]
                
                # ha nincsen buy
                if valid_records.empty:
                    hold_qty = 0
                    #close = data_train['close'].iloc[limit-minus_value]
                    #print(data_train['ticker'].iloc[limit-minus_value])
                    close = close_median
                    value = 0 # hold_qty * close = 0
                    cost = 0 # value * bp = 0
                else:
                    hold_qty = valid_records['qty'].sum()
                    # close = (valid_records['close'] * valid_records['qty']).sum()
                    #close = data_train['close'].iloc[limit-minus_value]
                    #print(data_train['ticker'].iloc[limit-minus_value])
                    close = close_median
                    value = valid_records['value'].sum()
                    cost = 0
            else:
                # hold_qty = transactions['qty'].iloc[last_sell_index:len(transactions)-1].sum(axis=1)
                valid_interval_data = transactions.iloc[last_sell_index:len(transactions)]

                # filter valid_interval_data where action == 1
                valid_records = valid_interval_data.loc[valid_interval_data['action'] == 1]

                # ha nincsen buy 
                if valid_records.empty:
                    hold_qty = 0
                    #close = data_train['close'].iloc[limit-minus_value]
                    #print(data_train['ticker'].iloc[limit-minus_value])
                    close = close_median
                    value = 0
                    cost = 0
                else:
                    # get sum of these records' qty
                    hold_qty = valid_records['qty'].sum()
                    # close = (valid_records['close'] * valid_records['qty']).sum()
                    #close = data_train['close'].iloc[limit-minus_value]
                    #print(data_train['ticker'].iloc[limit-minus_value])
                    close = close_median
                    value = hold_qty * close
                    cost = 0

            # current_balance nem változik

            # dict to record action parameters
            dicti = {'action': action, 'close': close, 'value': value, 'cost': cost, 'qty': hold_qty, 'crt_blc': current_balance, 'round': round}

            # convert dict to pandas Series
            s = pd.Series(dicti)

            # add Series to transactions df - df = df.append
            transactions = transactions.append(s, ignore_index=True)

            # calculate reward
            reward = agent.reward_function(a_t1=transactions['action'].iloc[len(transactions)-2] if len(transactions) >= 2 else 0, sigma_tgt=sigma_tgt, sigma_t1=data_train['sigma_t'].iloc[limit-2], r_t=data_train['r_t'].iloc[limit-1], bp=0.0020, p_t1=data_train['close'].iloc[limit-2], sigma_t2=data_train['sigma_t'].iloc[limit-3], a_t2=transactions['action'].iloc[len(transactions)-3] if len(transactions) >= 3 else 0)
            print('reward 0')
            print(reward)
            # reward = float(reward)
            
            done = False

            # false new observation_ for memory management
            false_batch_start = batch_start + 64
            false_batch_end = batch_end + 64

            false_limit = min(false_batch_end, L)

            false_batch_raw = data_train[false_batch_start:false_limit]

            false_batch = false_batch_raw[['close_normalized', 'volume_normalized']]

            observation_ = np.array(false_batch)
            observation_3d_ = observation.reshape(1, 64, 2)

            # store transition
            agent.store_transition(observation_3d, action, reward, observation_3d_, done)

            # run state through model
            agent.learn()

            # reset false_batch
            false_batch_raw = false_batch_raw.drop(false_batch_raw.index, inplace=True)
            false_batch_start = 0
            false_batch_end = 0

        # buy
        elif action == 1:
            # dict to record action parameters
            #close = data_train['close'].iloc[limit-minus_value]
            #print(data_train['ticker'].iloc[limit-minus_value])
            close = close_median

            buy_qty = math.floor((current_balance / 5) / close)
            value = buy_qty * close
            cost = value * bp

            current_balance = current_balance - (value + cost)

            dicti = {'action': action, 'close': close, 'value': value, 'cost': cost, 'qty': buy_qty, 'crt_blc': current_balance, 'round': round}

            # convert dict to pandas Series
            s = pd.Series(dicti)

            # add Series to transactions df - df = df.append
            transactions = transactions.append(s, ignore_index=True)

            # calculate reward - this is far from finished i think
            reward = agent.reward_function(a_t1=transactions['action'].iloc[len(transactions)-2] if len(transactions) >= 2 else 0, sigma_tgt=sigma_tgt, sigma_t1=data_train['sigma_t'].iloc[limit-2], r_t=data_train['r_t'].iloc[limit-1], bp=0.0020, p_t1=data_train['close'].iloc[limit-2], sigma_t2=data_train['sigma_t'].iloc[limit-3], a_t2=transactions['action'].iloc[len(transactions)-3] if len(transactions) >= 3 else 0)
            print('reward 0')
            print(reward)
            # reward = float(reward)

            done = False

            # false new observation_ for memory management
            false_batch_start = batch_start + 64
            false_batch_end = batch_end + 64

            false_limit = min(false_batch_end, L)

            false_batch_raw = data_train[false_batch_start:false_limit]

            false_batch = false_batch_raw[['close_normalized', 'volume_normalized']]

            observation_ = np.array(false_batch)
            observation_3d_ = observation.reshape(1, 64, 2)

            # store transition
            agent.store_transition(observation_3d, action, reward, observation_3d_, done)

            # run state through model
            agent.learn()
            
            # reset false_batch
            false_batch_raw = false_batch_raw.drop(false_batch_raw.index, inplace=True)
            false_batch_start = 0
            false_batch_end = 0

        # at the end get new batch (state), ez az eltolásos módszer
        batch_start += 64  
        batch_end += 64

        # record epsilon value 
        eps_history.append(agent.epsilon)
    round = round + 1

    print('current_balance:', current_balance)

print('Training done!')

"""## Debug training module"""

# %debug
print(agent.chosen_actions)

"""## Save trained model and transactions log"""

# god help me to save this shit
from google.colab import drive
drive.mount('/content/gdrive')

agent.save_model()

transactions_file = '/content/gdrive/My Drive/Colab Notebooks/saved_files/transactions_' + ticker + '.csv'
print('transactions_file:', transactions_file)

# save transactions and profit to file
transactions.to_csv(transactions_file, index=False)

# add current_balance to file
print(current_balance)

with open(transactions_file, 'a') as trans_file:
    text = '\n' + 'Current balance:' + str(current_balance)
    trans_file.write(text)
print('Write OK')

"""## Testing model"""

# mount drive
from google.colab import drive
drive.mount('/content/gdrive')

# load model
model = agent.load_model()
model.summary()

model.advantage

"""## Prepare test data (eltér a modell betanításához használt halmaztól)"""

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# AAPL
# data_link = 'https://drive.google.com/file/d/12KimzMtI95o4sccM9I9wvMWkDjP3V0J0/view?usp=sharing'
# id = '12KimzMtI95o4sccM9I9wvMWkDjP3V0J0'

# TSLA
# data_link = 'https://drive.google.com/file/d/1s2e95dBN1Hrw8muvFDf3_bbuet4w6UoP/view?usp=sharing'
# id = '1s2e95dBN1Hrw8muvFDf3_bbuet4w6UoP'

# AMD
# data_link = 'https://drive.google.com/file/d/1ljlYbzr4JfzhvJzzf-p_Pqi-TMJw8fff/view?usp=sharing'
# id = '1ljlYbzr4JfzhvJzzf-p_Pqi-TMJw8fff'

# MSFT
data_link = 'https://drive.google.com/file/d/1ZPgRkyye7uVTM-Hj0kJn7pR9BcASJRU5/view?usp=sharing'
id = '1ZPgRkyye7uVTM-Hj0kJn7pR9BcASJRU5'

downloaded = drive.CreateFile({'id':id})

data_file = 'MSFT_tech_Yahoo.csv'
downloaded.GetContentFile(data_file)

data = pd.read_csv(data_file)

# load test data
data_test = pd.DataFrame(data[1792:2560])

# reset index of df to start with 0
data_test = data_test.reset_index(drop=True)

print('...conventional individual stock data chosen...')
data_test = data_test.sort_values(by=['id'], ascending=True)

# normalize train data by adding normalized columns
data_normalizer = preprocessing.MinMaxScaler()

close_array_test = np.array(data_test['close'])
close_reshaped_test = close_array_test.reshape(-1, 1)

data_test['close_normalized'] = data_normalizer.fit_transform(close_reshaped_test)

volume_array_test = np.array(data_test['volume'])
volume_reshaped_test = volume_array_test.reshape(-1, 1)

data_test['volume_normalized'] = data_normalizer.fit_transform(volume_reshaped_test)

print('data_test head')
print(data_test.head())
print(data_test.tail())

"""## Helper variables and methods"""

L_test = len(data_test)

round_test = 0

# transactions made on test dataset
transactions_test = pd.DataFrame(columns=['action', 'close', 'value', 'cost', 'qty', 'crt_blc', 'round'])

# test model batch by batch
# reset budget
current_balance = 100000

"""## Test advantage, call and other methods"""

# creating testing agent
agent_test = Agent(lr=0.0001, gamma=0.3, n_actions=3, epsilon=1, batch_size=64, input_dims=(64,2), 
              epsilon_dec=1e-3, eps_end=0.01, mem_size=5000, fname=model_name,
              fc1_dims=64, fc2_dims=64, replace=1000, testing=True, model=model)

"""test tetst tsttststst"""

'''
def choose_action_test(state):
  # actions = agent.q_eval.predict(state)
  # na itten kell ez:
  actions = model.predict(state)
  print(actions)
  # actions = model.predict(state)
  action = tf.math.argmax(actions, axis=1).numpy()[0]
  action = np.mean(action)
  action = int(action.item())
  return action
'''

start = 0
end = 64
batch_raw = data_test[start:end]

# filter
batch = batch_raw[['close_normalized', 'volume_normalized']]
observation = np.array(batch)
observation_3d = observation.reshape(1, 64, 2)

# na itten jön az agent metódusa
# action = choose_action_test(observation_3d)
action = agent_test.choose_action(observation_3d, testing=True)
print(action)

# action = choose_action_test(observation_3d)

"""## Run pre-trained model on test data"""

batch_start = 0
batch_end = agent_test.batch_size

while batch_end <= L_test:
    limit = min(batch_end, L_test) # bruhhhh this is it

    print('batch_end:', batch_end)
    print('round_test:', round_test)
    
    # ha a limit az L-t választja, akkor azt jelenti, hogy végig értünk az adathalmazon, done = True
    # nem tanulunk itten
    if limit == L_test:
        print('limit:', limit, 'L:', L_test)
        batch_raw = data_test[batch_start:limit]

        # filter
        batch = batch_raw[['close_normalized', 'volume_normalized']]

        # okkay, i got the data - the state
        # convert it to numpy array, then it becomes the observation
        observation = np.array(batch)
        observation_3d = observation.reshape(1, 64, 2)

        action = agent_test.choose_action(observation_3d, testing=True)
        print('action in non-learning:', action)

        # sell
        if action == -1:
            # done = True
            transactions_test = transactions_test.sort_values(by=['round'], ascending=True)
            last_sell_index = transactions_test.loc[transactions_test['action'] == -1].last_valid_index()

            close = 0
            value = 0
            cost = 0
            qty = 0

            if last_sell_index == None:
                valid_records = transactions_test.loc[transactions_test['action'] == 1]

                if valid_records.empty:
                    close = data_test['close'].iloc[limit-1]
                    value = 0
                    cost = 0
                    qty = 0
                else:
                    # close = (valid_records['close'] * valid_records['qty']).sum()
                    # qty = valid_records['qty'].sum()
                    close = data_test['close'].iloc[limit-1]
                    qty = valid_records['qty'].sum()
                    value = qty * close
                    cost = value * bp
                    
                    current_balance = current_balance + (value - cost)

                    # profit += close


            else:
                valid_interval_data = transactions_test.iloc[last_sell_index:len(transactions_test)]

                # filter valid_interval_data where action == 1
                valid_records = valid_interval_data.loc[valid_interval_data['action'] == 1]

                if valid_records.empty:
                    close = 0
                    value = 0
                    cost = 0
                    qty = 0
                else:
                    # close = (valid_records['close'] * valid_records['qty']).sum()
                    # qty = valid_records['qty'].sum()
                    close = data_test['close'].iloc[limit-1]
                    qty = valid_records['qty'].sum()
                    value = qty * close
                    cost = value * bp
                    
                    current_balance = current_balance + (value - cost)

                    # profit += close

            # okkay do dict
            dicti = {'action': action, 'close': close, 'value': value, 'cost': cost, 'qty': qty, 'crt_blc': current_balance, 'round': round_test}

              # convert dict to pandas Series
            s = pd.Series(dicti)

            # add Series to transactions df - df = df.append
            transactions_test = transactions_test.append(s, ignore_index=True)

        # hold
        elif action == 0:
              # get current number of stocks (qty i hold = hold_qty)
            # get index of last sell
            transactions_test = transactions_test.sort_values(by=['round'], ascending=True)
            last_sell_index = transactions_test.loc[transactions_test['action'] == -1].last_valid_index()
            hold_qty = 0

            # close = value of held stocks
            close = 0
            value = 0
            cost = 0

            # last_sell_index == None: nem volt még eladás, akkor mi legyen
            if last_sell_index == None:
                # ha nincsen sell, akkor minden okés, az egész transactionst filterezzük
                valid_records = transactions_test.loc[transactions_test['action'] == 1]
                
                # ha nincsen buy
                if valid_records.empty:
                    hold_qty = 0
                    close = data_test['close'].iloc[limit-1]
                    value = 0
                    cost = 0
                else:
                    # hold_qty = valid_records['qty'].sum()
                    # close = (valid_records['close'] * valid_records['qty']).sum()
                    hold_qty = valid_records['qty'].sum()
                    close = data_test['close'].iloc[limit-1]
                    value = valid_records['value'].sum()
                    cost = 0
            else:
                # hold_qty = transactions['qty'].iloc[last_sell_index:len(transactions)-1].sum(axis=1)
                valid_interval_data = transactions_test.iloc[last_sell_index:len(transactions_test)]

                # filter valid_interval_data where action == 1
                valid_records = valid_interval_data.loc[valid_interval_data['action'] == 1]

                # ha nincsen buy 
                if valid_records.empty:
                    hold_qty = 0
                    close = data_test['close'].iloc[limit-1]
                    value = 0
                    cost = 0
                else:
                    # get sum of these records' qty
                    # hold_qty = valid_records['qty'].sum()
                    # close = (valid_records['close'] * valid_records['qty']).sum()
                    hold_qty = valid_records['qty'].sum()
                    close = data_test['close'].iloc[limit-1]
                    value = hold_qty * close
                    cost = 0

            # dict to record action parameters
            dicti = {'action': action, 'close': close, 'value': value, 'cost': cost, 'qty': hold_qty, 'crt_blc': current_balance, 'round': round_test}

            # convert dict to pandas Series
            s = pd.Series(dicti)

            # add Series to transactions df - df = df.append
            transactions_test = transactions_test.append(s, ignore_index=True)

        # buy
        elif action == 1:
            close = data_test['close'].iloc[limit-1]

            buy_qty = math.floor((current_balance / 5) / close)
            value = buy_qty * close
            cost = value * bp

            current_balance = current_balance - (value + cost)

            # dict to record action parameters
            dicti = {'action': action, 'close': close, 'value': value, 'cost': cost, 'qty': buy_qty, 'crt_blc': current_balance, 'round': round_test}

            # convert dict to pandas Series
            s = pd.Series(dicti)

            # add Series to transactions df - df = df.append
            transactions_test = transactions_test.append(s, ignore_index=True)

        break
    
    # a limit belefér az adathalmazban itt folytathatjuk a batch-et, done = False
    else:
        batch_raw = data_test[batch_start:limit]

        # filter
        batch = batch_raw[['close_normalized', 'volume_normalized']]

        observation = np.array(batch)

        observation_3d = observation.reshape(1, 64, 2)

        # berakni a modellbe, várni az outputot
        action = agent_test.choose_action(observation_3d, testing=True)
        print('action in learning:', action)

        # sell
        if action == -1:
            # sell everything back to the last sell that was a buy
            # get index of last sell
            transactions_test = transactions_test.sort_values(by=['round'], ascending=True)
            print('transactions_test df in -1')
            print(transactions_test)
            last_sell_index = transactions_test.loc[transactions_test['action'] == -1].last_valid_index()
            print('last_sell_index:', last_sell_index)

            close = 0
            value = 0
            cost = 0
            qty = 0

            # még nem volt eladás
            if last_sell_index == None:
                valid_records = transactions_test.loc[transactions_test['action'] == 1]
                
                # nincsen vétel sem
                if valid_records.empty:
                    close = data_test['close'].iloc[limit-1]
                    value = 0
                    cost = 0
                    qty = 0
                else:
                    # close = (valid_records['close'] * valid_records['qty']).sum()
                    close = data_test['close'].iloc[limit-1]
                    qty = valid_records['qty'].sum()
                    value = qty * close
                    cost = value * bp
                    
                    current_balance = current_balance + (value - cost)

                    # profit += close


            else:
                valid_interval_data = transactions_test.iloc[last_sell_index:len(transactions_test)] #-1

                # filter valid_interval_data where action == 1
                valid_records = valid_interval_data.loc[valid_interval_data['action'] == 1]

                if valid_records.empty:
                    close = 0
                    value = 0
                    cost = 0
                    qty = 0
                else:
                    # close = (valid_records['close'] * valid_records['qty']).sum()
                    close = data_test['close'].iloc[limit-1]
                    qty = valid_records['qty'].sum()
                    value = qty * close
                    cost = value * bp
                    
                    current_balance = current_balance + (value - cost)

                    # profit += close

            # okkay do dict
            dicti = {'action': action, 'close': close, 'value': value, 'cost': cost, 'qty': qty, 'crt_blc': current_balance, 'round': round_test}

              # convert dict to pandas Series
            s = pd.Series(dicti)

            # add Series to transactions df - df = df.append
            transactions_test = transactions_test.append(s, ignore_index=True)

        # hold
        elif action == 0:
            # get current number of stocks (qty i hold = hold_qty)
            # get index of last sell
            transactions_test = transactions_test.sort_values(by=['round'], ascending=True)
            last_sell_index = transactions_test.loc[transactions_test['action'] == -1].last_valid_index()
            hold_qty = 0

            # close = value of held stocks
            close = 0
            value = 0
            cost = 0

            # last_sell_index == None: nem volt még eladás, akkor mi legyen
            if last_sell_index == None:
                # ha nincsen sell, akkor minden okés, az egész transactionst filterezzük
                valid_records = transactions_test.loc[transactions_test['action'] == 1]
                
                # ha nincsen buy
                if valid_records.empty:
                    hold_qty = 0
                    close = data_test['close'].iloc[limit-1]
                    value = 0 # hold_qty * close = 0
                    cost = 0 # value * bp = 0
                else:
                    hold_qty = valid_records['qty'].sum()
                    # close = (valid_records['close'] * valid_records['qty']).sum()
                    close = data_test['close'].iloc[limit-1]
                    value = valid_records['value'].sum()
                    cost = 0
            else:
                # hold_qty = transactions['qty'].iloc[last_sell_index:len(transactions)-1].sum(axis=1)
                valid_interval_data = transactions_test.iloc[last_sell_index:len(transactions_test)]

                # filter valid_interval_data where action == 1
                valid_records = valid_interval_data.loc[valid_interval_data['action'] == 1]

                # ha nincsen buy 
                if valid_records.empty:
                    hold_qty = 0
                    close = data_test['close'].iloc[limit-1]
                    value = 0
                    cost = 0
                else:
                    # get sum of these records' qty
                    hold_qty = valid_records['qty'].sum()
                    # close = (valid_records['close'] * valid_records['qty']).sum()
                    close = data_test['close'].iloc[limit-1]
                    value = hold_qty * close
                    cost = 0

            # current_balance nem változik

            # dict to record action parameters
            dicti = {'action': action, 'close': close, 'value': value, 'cost': cost, 'qty': hold_qty, 'crt_blc': current_balance, 'round': round_test}

            # convert dict to pandas Series
            s = pd.Series(dicti)

            # add Series to transactions df - df = df.append
            transactions_test = transactions_test.append(s, ignore_index=True)


        # buy
        elif action == 1:
            # dict to record action parameters
            close = data_test['close'].iloc[limit-1]

            buy_qty = math.floor((current_balance / 5) / close)
            value = buy_qty * close
            cost = value * bp

            current_balance = current_balance - (value + cost)

            dicti = {'action': action, 'close': close, 'value': value, 'cost': cost, 'qty': buy_qty, 'crt_blc': current_balance, 'round': round_test}

            # convert dict to pandas Series
            s = pd.Series(dicti)

            # add Series to transactions df - df = df.append
            transactions_test = transactions_test.append(s, ignore_index=True)

        # at the end get new batch (state), ez az eltolásos módszer
        batch_start += 1
        batch_end += 1

        # lets not record epsilon value 
        # eps_history.append(agent.epsilon)
    round_test = round_test + 1

    print('current_balance:', current_balance)

print('Testing done!')

"""## Save test results and transactions log"""

from google.colab import drive
drive.mount('/content/gdrive')

# agent.save_model()

# transactions_test_file = '/content/gdrive/My Drive/Colab Notebooks/saved_files/transactions_test_' + ticker + '.csv'
transactions_test_file = '/content/gdrive/My Drive/Colab Notebooks/saved_files/transactions_test_MSFT_tech_model.csv'

# save transactions and profit to file
transactions_test.to_csv(transactions_test_file, index=False)

# add current_balance to file
print(current_balance)

with open(transactions_test_file, 'a') as trans_file:
    text = '\n' + 'Current balance:' + str(current_balance)
    trans_file.write(text)
print('Write OK')
